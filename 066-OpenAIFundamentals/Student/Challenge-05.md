# Reto 05: IA Responsable

[< reto Anterior](./Challenge-04.md) - **[Home](../README.md)**

## Prerrequisitos

- Implementación del índice de búsqueda cognitiva de reto 4.

## Introducción

A medida que los Modelos de Lenguaje de Gran Tamaño (LLMs) crecen en popularidad y uso en todo el mundo, la necesidad de gestionar y monitorear sus resultados se vuelve cada vez más importante. En este reto, aprenderás cómo evaluar los resultados de los LLMs y cómo identificar y mitigar posibles sesgos en el modelo.

## Descripción

Preguntas que deberías poder responder al final de este reto:
- ¿Qué servicios y herramientas existen para identificar y evaluar daños y fugas de datos en LLMs?
- ¿Cuáles son las formas de evaluar la veracidad y reducir las alucinaciones?
- ¿Qué métodos existen para evaluar un modelo si no tienes un conjunto de datos de verdad absoluta para comparar?

Secciones en este reto:
1. Identificación de daños y detección de Información Personal Identificable (PII)
2. Evaluación de la veracidad utilizando Conjuntos de Datos de Verdad Absoluta
3. Evaluación de la veracidad utilizando GPT sin Conjuntos de Datos de Verdad Absoluta

Ejecutarás el siguiente Jupyter Notebook para este reto:

- `CH-05-ResponsibleAI.ipynb`

El archivo se puede encontrar en tu Codespace bajo la carpeta `/notebooks`.

Regresa aquí a la guía del estudiante después de completar todas las tareas en el Jupyter Notebook para validar que has cumplido con los criterios de éxito para este reto.

## Criterios de Éxito

Para completar este reto con éxito, deberías ser capaz de:
- Articular los principios de IA Responsable con OpenAI.
- Demostrar métodos y enfoques para evaluar LLMs.
- Identificar herramientas disponibles para identificar y mitigar daños en LLMs.

## Recursos Adicionales

- [Visión General de las Prácticas de IA Responsable para Modelos de Azure OpenAI](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)
- [Servicios Cognitivos de Azure - Qué es el Filtrado de Contenido](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/content-filter)
- [Herramienta de Seguridad de Contenido de Azure AI](https://learn.microsoft.com/en-us/azure/cognitive-services/content-safety/overview)
- [Función de Anotaciones de Seguridad de Contenido de Azure](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/content-filter#annotations-preview)
- [Plugin de Detección de PII de OpenAI](https://github.com/openai/chatgpt-retrieval-plugin/tree/main#plugins)
- [Biblioteca Evaluate de Hugging Face](https://huggingface.co/docs/evaluate/index)
- [Documento Técnico de OpenAI sobre Evaluación de Modelos (ver Sección 3.1)](https://cdn.openai.com/papers/gpt-4-system-card.pdf)


## Agradecimientos Finales

¡Muchas gracias por haber sido parte de este desafío sobre OpenAI y Azure que preparamos para ti hoy! Estamos muy felices de que hayas llegado hasta el final y queremos reconocer tu esfuerzo. Como agradecimiento, tenemos un regalo especial para ti. Por favor, busca a alguien de nuestro equipo para recibirlo.

Nos encantaría saber qué te pareció este taller. Te invitamos a completar este [formulario anónimo](https://docs.google.com/forms/d/e/1FAIpQLSfQNftvozuCRmnuSOvPWaa6hYvc0FfUxu7VH6wLUuJ8upOU2A/viewform). Tu opinión es muy valiosa y nos tomará solo dos minutos de tu tiempo. Tus comentarios nos ayudarán a mejorar y a seguir ofreciendo experiencias enriquecedoras en futuras ediciones.

Finalmente, nuestros amigos de Microsoft desean recordarte que tienen un newsletter increíble para desarrolladores. Si estás interesado en mantenerse al tanto de las novedades que tienen para ofrecer, puedes suscribirte [aquí](https://aka.ms/AAqcy98).

¡Gracias una vez más por tu participación y esperamos verte en futuros eventos!
