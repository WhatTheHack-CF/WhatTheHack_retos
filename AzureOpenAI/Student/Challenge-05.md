# Reto 05: IA Responsable

[< Reto Anterior](./Challenge-04.md) - **[Home](../README.md)**

## Prerrequisitos

- Implementación del Índice de AI Search del reto 4.

## Introducción

A medida que los Modelos de Lenguaje de Gran Tamaño (LLMs) crecen en popularidad y uso en todo el mundo, la necesidad de gestionar y monitorear sus resultados se vuelve cada vez más importante. En este reto, aprenderás cómo evaluar los resultados de los LLMs y cómo identificar y mitigar posibles sesgos en el modelo.

## Descripción

### Preguntas que deberías poder responder al final de este reto:
- ¿Cuáles son los servicios y herramientas que existen para identificar y evaluar daños y la fuga de datos en los LLMs?
- ¿Cuáles son las formas de evaluar la veracidad y reducir las fabricaciones?
- ¿Qué métodos existen para evaluar un modelo si no tienes un conjunto de datos de verdad absoluta para comparar?

### Secciones en este reto:
1. Identificación de daños y detección de Información Personal Identificable (PII)
2. Evaluación de la veracidad utilizando Conjuntos de Datos de Verdad Absoluta
3. Evaluación de la veracidad utilizando GPT sin Conjuntos de Datos de Verdad Absoluta

### Tareas a realizar
Ejecutarás el siguiente Jupyter Notebook para este desafío:

- `CH-05-ResponsibleAI.ipynb`

El archivo se puede encontrar en tu Codespace bajo la carpeta `/notebooks`.

Regresa aquí después de completar todas las tareas en el Jupyter Notebook para validar que has cumplido con los criterios de éxito para este desafío.

## Criterios de Éxito

Para completar este reto con éxito, deberías ser capaz de:
- Articular los principios de IA Responsable con OpenAI.
- Demostrar métodos y enfoques para evaluar LLMs.
- Identificar herramientas disponibles para identificar y mitigar daños en LLMs.

## Recursos Adicionales

- [Descripción General de las Prácticas de IA Responsable para los Modelos de Azure OpenAI](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)
- [Servicios de IA de Azure - Qué es el Filtrado de Contenido](https://learn.microsoft.com/es-mx/azure/cognitive-services/openai/concepts/content-filter)
- [¿Qué es Azure Content Safety?](https://learn.microsoft.com/es-mx/azure/cognitive-services/content-safety/overview)
- [Función de Anotaciones en Azure Content Safety](https://learn.microsoft.com/es-mx/azure/cognitive-services/openai/concepts/content-filter#annotations-preview)
- [Plugin de Detección de PII de OpenAI](https://github.com/openai/chatgpt-retrieval-plugin/tree/main#plugins)
- [Biblioteca Evaluate de Hugging Face](https://huggingface.co/docs/evaluate/index)
- [Documento Técnico de OpenAI sobre Evaluación de Modelos (ver Sección 3.1)](https://cdn.openai.com/papers/gpt-4-system-card.pdf)

## Agradecimientos Finales

¡Muchas gracias por haber sido parte de este desafío sobre OpenAI y Azure que preparamos para ti hoy! Estamos muy felices de que hayas llegado hasta el final.

**¡Felicidades! ¡¡Haz concluido el What The Hack Fundamentos de Azure OpenAI!!**

¡Gracias una vez más por tu participación y esperamos verte en futuros eventos!
